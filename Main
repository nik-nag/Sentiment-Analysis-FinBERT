from bs4 import BeautifulSoup
from urllib.request import Request, urlopen
import pandas as pd
import time
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
import mysql.connector
from mysql.connector import Error

class SentimentAnalysisBot:
    def __init__(self, tickers, host, user, password, database, timer=60):
        self.url = "https://finviz.com/quote.ashx?t="
        self.tickers = tickers
        self.filter_array = []
        self.database_datetime = []
        self.connection = self.create_connection(host, user, password, database)
        self.my_query = '''INSERT INTO StockMessages (datetime, stock_ticker, message) VALUES (%s, %s, %s);'''
        self.current = time.monotonic()
        self.timer = timer

        # Load the model and tokenizer for sentiment analysis
        self.tokenizer = AutoTokenizer.from_pretrained("ProsusAI/finbert")
        self.model = AutoModelForSequenceClassification.from_pretrained("ProsusAI/finbert")
        self.pipe = pipeline("text-classification", model=self.model, tokenizer=self.tokenizer)

    def create_connection(self, host_name, user_name, user_password, data_base):
        try:
            connection = mysql.connector.connect(
                host=host_name,
                user=user_name,
                password=user_password,
                database=data_base
            )
            print("Connection to MySQL DB successful")
            return connection
        except Error as e:
            print(f"The error '{e}' occurred")
            return None

    def execute_query(self, query, symbol, date, news):
        cursor = self.connection.cursor()
        try:
            cursor.execute(query, (date, symbol, news))
            self.connection.commit()
            print(f"Query successfully sent to database: {symbol} | {date} | {news}")
        except Error as e:
            print(f"The error '{e}' occurred")

    def web_scrape(self):
        data_array = []
        time_array = []
        ticker_array = []
        for symbol in self.tickers:
            fin_url = self.url + symbol
            request = Request(url=fin_url, headers={'user-agent': 'home'})
            response = urlopen(request)
            html = BeautifulSoup(response, features='html.parser')
            news = html.find(id="news-table")
            data_rows = news.findAll('tr')
            recent_data = data_rows[0].a.text
            recent_time = data_rows[0].td.text.replace('\r', '').replace('\n', '').strip()
            print(f'Data scraped: {symbol} | {recent_time} | {recent_data}')
            data_array.append(recent_data)
            time_array.append(recent_time)
        return data_array, time_array

    def filter_data(self, data_arr, time_arr):
        for i in range(len(data_arr)):
            if [self.tickers[i], time_arr[i], data_arr[i]] not in self.filter_array:
                self.filter_array.append([self.tickers[i], time_arr[i], data_arr[i]])
        if len(self.filter_array) == 10:
            del self.filter_array[0]
        return self.filter_array

    def sentiment_analysis(self, arr):
        data = arr[-1][-1]
        result = self.pipe(data)
        print('Analyzing recent news for sentiment...')

        if result[0]['label'] != 'neutral' and result[0]['score'] >= 0.8 and arr[-1][-1] not in self.database_datetime:
            print("High sentiment financial news found...")
            self.database_datetime.append(arr[-1][-1])
            self.execute_query(self.my_query, arr[-1][0], arr[-1][1], arr[-1][2])

        if len(self.database_datetime) == 20:
            self.database_datetime = []

    def run(self):
        while True:
            if time.monotonic() > self.current + self.timer:
                data_arr, time_arr = self.web_scrape()
                filtered_data = self.filter_data(data_arr, time_arr)
                self.sentiment_analysis(filtered_data)
                self.current = time.monotonic()

# Example usage
if __name__ == "__main__":
    tickers = ["AAPL", "MSFT", "GOOGL"]
    host = "localhost"
    user = "root"
    password = "INSERT_PASSWORD_HERE"
    database = "INSERT_DATABASE_NAME_HERE"
    bot = SentimentAnalysisBot(tickers, host, user, password, database, timer=15)
    bot.run()
